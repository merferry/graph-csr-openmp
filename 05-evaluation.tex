\subsection{Experimental Setup}
\label{sec:setup}

We use a server that has two $16$-core x86-based Intel Xeon Gold 6226R processors running at $2.90$ GHz. Each core has an L1 cache of $1$ MB, an L2 cache of $16$ MB, and a shared L3 cache of $22$ MB. The machine has $93.4$ GB of system memory and runs on CentOS Stream 8. We use GCC 8.5 and OpenMP 4.5. Table \ref{tab:dataset} shows the graphs we use in our experiments. All of them are obtained from the SuiteSparse Matrix Collection \cite{kolodziej2019suitesparse}.

\input{src/tab-dataset}

\input{src/fig-runtime}
\input{src/fig-scaling}
\input{src/fig-compare-large}
\input{src/fig-compare-small}


In our comparison with Hornet, Gunrock, and PIGO, we observe that:

PIGO has demonstrated superiority over XXX, prompting our exclusion from the comparison.
The average speedup over Gunrock showcases the efficiency of our approach.
Similarly, the average speedup over PIGO further highlights the performance benefits of our method.
Notably, our approach excels on web graphs characterized by power law distributions and high average degrees.
We present our key findings from the observations in the figure captions.


\textbf{Figure 3}:

The efficiency of reading an edgelist stands out, significantly outpacing the speed of reading a graph stored as CSR, which necessitates an additional conversion step. Surprisingly, the average cost of converting an edgelist to CSR is three times higher than simply reading an edgelist from a file. This underscores the notable speed of modern I/O processes. In the sk-2005 dataset, our approach achieves an impressive reading speed of 1.9 billion edges per second. Notably, it's essential to clarify that the table represents edges as directed, not undirected. To visually represent the edge reading performance across different graphs, we propose the inclusion of a plot illustrating this metric. The sluggishness in CSR reading is justified by its nature as a shared data structure, leading to high contention when multiple threads concurrently attempt to add an edge to the same vertex. The process of adding to CSR structures with small degrees is hampered by false sharing, introducing cache coherency issues that contribute to decreased performance.

We observe that:
- reading edgelist is significantly faster than reading a graph as CSR (this requires and additional conversion step)
- in fact, on average, converting an edgelist to CSR is 3 times more costly than reading and edgelist from file (which is quite surprising indded). This shows that modern IO is fastr (dhooom).
- We are able to read 1.9B edges per second (the table put edges as directed not undirected) in sk-2005.
- Can we have a plot on edge reading performance per graph
- (Justify why CSR reading is slow) converting to CSR is slow becuase it is a shared data structure resulting in high contention beteen thread, when multiple threads attempt to add an edge to the same vertex.
- Also adding to CSR with small degrees suffer from false sharing (cache coherency issue).

\textbf{Figure 4}:



Our observations reveal:

Reading edgelists exhibits impressive scalability, achieving a remarkable 25x scale on 32 threads.
However, at 64 threads, performance is impacted by NUMA effects.
Notably, reading CSR does not scale as well, primarily due to issues such as false sharing and contention.
The influence of NUMA effects is also discernible in CSR reading performance.
The scaling pattern follows multiples of 2 (linear Y-axis, logarithmic X-axis).

% There is a paper called "Efficient Memory Mapped File I/O for In-Memory File Systems" on the topic - where Choi et al. working at Samsung say that mmap() is a good interface for fast IO (in contrast to file streams) and propose async map-ahead based madvise() for low-latency NVM storage devices. The also have a good slides on this - where their (modified) extended madvice obtains ~2.5x better performance than default mmap() by minimizing the number of page faults.
% I tried parsing integers from text and saving into per-thread integer lists. To over-allocate memory for per-thread integer-lists i use sufficient size mmap() instead of malloc(). Even if i over-allocate, due to virtual memory, only memory as much i need is actually used.
