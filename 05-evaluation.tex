% EMAIL
Gunrock is the slowest here, followed by cuhornet, the one i am working on, and PIGO. PIGO is still ~8x faster than my implementation, but it does not support dynamic updates while my implementation does. Reducing loading time in important to enable faster iterations upon our algorithms.
My implementation inserts into a 2d-vector (or a vector of vectors) and ensures sorted and unique keys upon call to update(). This is faster than doing a binary search upon each addEdge() to check for an edge's existence. I use a custom unique\_merge() function for this - using a small buffer (a bufferless merge is inefficient).

I tried a simple file bytes sum using mmap() - both sequential and parallel using openmp (64 threads on DGX). I adjust madvise(), mmap(), and per-thread block size to see which access pattern has the best performance.
It seems to me using an early madvice(MADV\_WILLNEED), a per-thread block size of 256K (dynamic schedule) is good. Below is a plot showing the time taken with this config for each graph.

In parallel doing a file byte sum takes ~100ms on indochina-2004 graph. Note that PIGO takes ~650ms to load the same graphs as CSR. Next i measure the read bandwith of each file, simply by dividing the size of each file by the time taken.

We appear to be reaching a peak bandwidth of ~35GB/s. The KIOXIA KCM6DRUL7T68 7.68TB NVMe SSD installed on DGX has a peack sequential read performance of 62GB/s (we are close). Sequential approach can hit a max of only 6GB/s.
https://www.acmemicro.com/Product/17847/Kioxia-KCD6XLUL7T68---7-68TB-SSD-NVMe-2-5-inch-15mm-CD6-R-Series-SIE-PCIe-4-0-6200-MB-sec-Read-BiCS-FLASH-TLC-1-DWPD

There is a paper called "Efficient Memory Mapped File I/O for In-Memory File Systems" on the topic - where Choi et al. working at Samsung say that mmap() is a good interface for fast IO (in contrast to file streams) and propose async map-ahead based madvise() for low-latency NVM storage devices. The also have a good slides on this - where their (modified) extended madvice obtains ~2.5x better performance than default mmap() by minimizing the number of page faults.

I tried parsing integers from text and saving into per-thread integer lists. To over-allocate memory for per-thread integer-lists i use sufficient size mmap() instead of malloc(). Even if i over-allocate, due to virtual memory, only memory as much i need is actually used.


\textbf{Figure 3}:

We observe that:
- reading edgelist is significantly faster than reading a graph as CSR (this requires and additional conversion step)
- in fact, on average, converting an edgelist to CSR is 3 times more costly than reading and edgelist from file (which is quite surprising indded). This shows that modern IO is fastr (dhooom).
- We are able to read 1.9B edges per second (the table put edges as directed not undirected) in sk-2005.
- Can we have a plot on edge reading performance per graph
- (Justify why CSR reading is slow) converting to CSR is slow becuase it is a shared data structure resulting in high contention beteen thread, when multiple threads attempt to add an edge to the same vertex.
- Also adding to CSR with small degrees suffer from false sharing (cache coherency issue).

\textbf{Figure 4}:

We observe that:
- Reading edelis tscales well - scaling upto 25x on 32 threads (amazing).
- However, at 64 thread, NUMA edffects kick in, killing performance.
- Note that reading CSR does to scale as good. This is mainly to due the issues mentioned above, i.e., false sharing and contention.
- NUMA effects are also oberved here.
- Scaling in multiple of 2 (Y axis is linear, X is logarithmic).

\textbf{Figure 5, 6}:

We obsevre that:
- Here we compare performance of GVEL (our approach) with Hornet, Gunroack, and PIGO.
- We do not compre with XXX becuase PIGO has already show superioty over them.
- Add obervation from caption.
- Average speedup over Gunrock...
- Average speedup over PIGO
- Our approach is especially faster on web graphs which exhibit power law and high avg degrees.
