The choice of graph file format affects loading speed. COO (Coordinate List), MTX (Matrix Market), and other formats have different characteristics, and selecting an appropriate format based on the application and storage considerations is crucial. Memory storage formats, such as Edgelist and CSR (Compressed Sparse Row), impact the efficiency of graph processing algorithms. Choosing the right format can lead to significant performance improvements during both loading and computation.

% - File formats
% - In-memory formats
% - Memory map

\subsection{Mmemory Mampped IO}
\label{sec:mmio}

In modern operating systems, memory-mapped I/O (mmio) is an important access method that maps a file or file-like resource to a region of memory. The mapping allows applications to access data from files through memory semantics (i.e., load/store) and it provides ease of programming. The number of applications that use mmio are increasing because memory semantics can provide better performance than file semantics (i.e., read/write). As more data are located in the main memory, the performance of applications can be enhanced owing to the effect of a large cache. When mmio is used, hot data tend to reside in the main memory and cold data are located in storage devices such as HDD and SSD; data placement in the memory hierarchy depends on the virtual memory subsystem of the operating system. Generally, the performance of storage devices has a direct impact on the performance of mmio. It is widely expected that better storage devices will lead to better performance. mmap can effectively extend the main memory with fast storage devices \cite{song2016efficient}.

Memory-mapped I/O (mmio) is emerging as a viable alternative for accessing directly-attached fast storage devices compared to explicit I/O with system calls. Mmio removes the need for costly lookups in the DRAM I/O cache for cache hits, as they are handled in hardware via the virtual memory mechanism \cite{malliotakis2021hugemap}.

Memory-mapped I/O provides several potential advantages over explicit read/write I/O, especially for low latency devices: (1) It does not require a system call, (2) it incurs almost zero overhead for data in memory (I/O cache hits), and (3) it removes copies between kernel and user space \cite{papagiannis2020optimizing}.

With current technology trends for fast storage devices, the host-level I/O path is emerging as a main bottleneck for modern, data-intensive servers and applications. The need to improve I/O performance requires customizing various aspects of the I/O path, including the page cache and the method to access the storage devices \cite{papagiannis2021memory}.

Memory-mapped I/O, which internally uses demand paging, has recently become popular when paired with low-latency storage. It improves I/O performance by mapping the data DMA transfers directly to userspace memory and removing the additional data copy between user/kernel space \cite{li2019userland}.

Kernel FS is a huge overhead for fast storage \cite{yoshimura2019evfs}.

\input{src/tab-dataset}
