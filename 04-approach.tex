State our focus of COO and MTX formats (but we study only MTX format in these experiments).
We appropriately handle the header of the file format.

The mmap interface is faster than the traditional read/write because it avoids the system call overhead and copies between the kernel and user space. However, I/O with mmap is synchronous and has more complex error handling. For instance, on a disk full a write would return with an error code, while mmap-based I/O would require handling a signal \cite{enberg2022transcending}.

In contrast, Direct I/O (DIO) allows applications to use the same read and write system calls while bypassing the page cache. However the buffer management and caching is performed in user space \cite{enberg2022transcending}.




\subsection{Reading Edgelist from text file}

\textbf{approach-read-stream-plain}:

In this approach, we use C++'s ifstream to open the file, and read the edges line by line. If the graph is unweighted, we simply read the source and target integers as sizet into source and target arrays that are pre-allocated based on information in the header of the file (rows, cols, size). If the graph is weighted, we also read the weights as 32-bit floating point numbers into another preallocated array. We do this seqeuntiall as the stream sare inherently sequential. Is this buffered, please check? Why are C++'s stream slow? virtual calls?

\textbf{approach-read-stream-strtoull}

In this approach, we again use C++'s ifstream - but use string to number conversion methods strtoull and strtod to perform number parsing in parallel. For this, we first read a chunk of 128K lines from the file sequentially from ifstream using getline, and then parse each line using multiple threads in parallel (with OpenMP's dynamic scheduling with chunk size of 1024, which we observe performs well). Once the edges are parsed (source, target, weight), we store them separately in per thread edge-lists to avoid contention issue into a shared data structure. With 64 threads, this approach provided a speed improvement of XX, compared to approach-read-stream-plain.

\textbf{approach-read-fscanf-plain}

This appraoch is ismilar to the approach mentioned above, but we use fgets on a file handle, instead of ifstream >> operator, and use sscanf to parse the edges (source, target, weight - if weighted). This approach can be performed in parallel. With 64 threads, this approach provede a speedup of X wrt to X.

\textbf{approach-read-fscanf-strtoull}

This approach is similar to the approach above, but use use strtoull and strtod instead of sscanf. This is faster due to the absence of having to parse a format string. With 64 threads, this approach provide a speedu pof X over X.

\textbf{approach-read-mmap-sscanf}

It is possible to read chunks of a file using fread, and process them in parallel. However, such reads can only be performed sequentially, requires special logic and additional buffers to keep track of partial lines and both ends of a chunk, requires synchronization between threads, and data loaded into memory needs to be swapped out to disk in case of low memory. This is in contrast to memory mapped files, which use mmap, to which map files to the mmroy space b yusing the paging mechanism  -provided my all modern processors supporting virtual memory. In fact it is the paging mechanism. Here, a initial mapping of memory address range to the file is ceated by the kernel. When a thread access a memory that is not loaded from disk, a page fault occurs and the page (4K) is loaded into memory and the page table is updated. The kernel can unload the page instantly if the system is low on memory (if no writes have been made to the page). With memory mapping, extremenly large file can be easily mapped to memory, thanks to 64-bit memory addresses, and conviniently processed by threads. In additiona requenct to load pages can be made in parallel by multiple threads. This, in addition to a number of other reasons, makes mmap an attractive choice for IO.
[Maybe move this to preliminaries]

We can also use mmap to allocate memory for the current process. The kernel ensures that the allocated meory is zero-filled upon use.

madive can be used to instruct the kernel about the page access pattern in order to optimize loading of pages. We observe that an madvaise of eiki gives good performance. We also observe that using a chunk size of 256K gives goood parallel performance with memory mapped files. This is what is use in the rest of the report.
(We use file bytes sum experiment for this - take some details from there).

For process edges in parallel using multiples thread (64), we assign chuncks of size 256K to each thread. Each thread works as follows. ItEach thread processes the chunk assign to it sequentilly. First, if the chunk has partial lines and the begin and at the end of the chunk, it re posistion the chunk such that the chunk had no partial lines (use BLOCK term, not chunk). To do this, it skips the partial line at the beign of the chunk, and the includes partial line from the end of the chunk. Then it procees the lines (edges) sequentially.

To process the edges, we use sscanf, strttoull/strtod, and a custom number parsing function for both whole numbers and floats. The last approach gives us the best performance as shown in figure X. We also attempt using custom SIMD sintructions to parse integers in parallel along with vzeroupper to cleasr sse/avx register to avoid performance peonalty. Unfortunatele, this provied us with no additional improve ment in performance. Finally we decrement a BASE values from the parse number to make the vertex-ids zero-based. The relative runtime is hsown in Figure X. This needs to be done appropriately, otherwise there can be a large performance penaltl. Using weighted flag as a template arguments allowed us to improve imporfance - the loop code needs to be tight to fit in the cache.

Maybe now mention the other slow approaches?
The algorithm for this approach is given in Algoriths X.

\input{src/alg-el}
\input{src/alg-csr}
\input{src/fig-optimize-el}
\input{src/fig-optimize-csr}




\subsection{Converting Edgelist to CSR}

Note that we earlier obtained per-thread edgelists. We must now convert the edgelists to CSR. To do this, we first need to know the degree of each vertex. A simple solution to this is to update the degree of each vertex while loading a graph. We observe that this relsults in high contention and impact performance. In contrast, we can compute per-thread degrees. However, this a large number of additional space, and needs to be combined later to obtain global degrees. We oberve that computing degrees in partitons of 4 (using mod 4) and then combined them gives us the best perofrmance.

Now that we have the degrees, we need to combine the edges in to a CSR structure. Unfortunaltely, again using a gloval CSR has poor performance. Accordingly, we explore computing CSR in k partiones and later combining them. Our observations indicate that using 4 partitons to generate CSR and then combining them has the best operformance. Now describe the figures and the algorithm.

\input{src/fig-runtime}
\input{src/fig-scaling}
\input{src/fig-compare-large}
\input{src/fig-compare-small}
