Graph processing frameworks are software systems designed to efficiently analyze and manipulate graph-structured data, facilitating tasks such as traversal, analytics, and algorithm implementation on various computational architectures. Ligra \cite{shun2013ligra} is a lightweight framework designed for shared-memory parallel/multicore machines, simplifying the development of graph traversal algorithms with two simple routines for edge and vertex mapping. The Galois system \cite{nguyen2013lightweight} advocates for a general-purpose infrastructure supporting fine-grain tasks, speculative execution, and application-specific task scheduling control. Gunrock \cite{wang2016gunrock} tackles irregular data access and control flow issues on GPUs by introducing a data-centric abstraction focused on vertex or edge frontier operations, and enables rapid development of new graph primitives with minimal GPU programming knowledge. Hornet \cite{busato2018hornet} provides a scalable GPU implementation without requiring data reallocation during evolution, targeting dynamic data problems. These frameworks contribute technically to enhancing the efficiency and programmability of graph processing across different computational architectures. While these frameworks have demonstrated efficient computation on billion-scale graphs, they persist with sequential I/O and do not fully exploit the capabilities of modern Hard Disk Drives (HDDs), Redundant Array of Independent Disks (RAID) controllers, Non-Volatile Memory (NVM), and file system caches \cite{gabert2021pigo}.

Incorporation of Memory-Mapped I/O holds promise for improving graph loading efficiency of frameworks. A large number of works have addressed 



In the domain of graph processing frameworks, a range of studies has been conducted to enhance performance and efficiency. Papagiannis et al. \cite{papagiannis2021memory} introduce Aquila, a library OS facilitating customizable memory-mapped I/O (mmio) paths for files or storage devices, demonstrating notable advantages in key-value stores and graph processing applications. Leis et al. \cite{leis2023virtual} propose vmcache, a buffer manager design that utilizes hardware-supported virtual memory for efficient page identifier translation to virtual memory addresses, offering flexible, efficient, and scalable buffer management on multi-core CPUs and fast storage devices. Feng et al. \cite{feng2023tricache} present TriCache, a cache mechanism enabling in-memory programs to process out-of-core datasets efficiently through a multi-level block cache design. Additionally, Song et al. \cite{song2012low} investigate the Linux virtual memory subsystem and mmap() I/O path for low-latency storage devices, proposing optimization policies to mitigate overheads. Imamura and Yoshida \cite{imamura2019poster} propose AR-MMAP, an asynchronous read method reducing write latency in memory-mapped files. Li et al. \cite{li2019userland} propose CO-PAGER, a lightweight userspace memory service offering customizable and efficient memory paging. These studies collectively contribute to the optimization and advancement of graph processing frameworks, addressing challenges and enhancing performance in various computational contexts.

%%
The literature on memory-mapped I/O (mmap) optimization in the context of high-performance computing (HPC) reveals various approaches to enhance graph loading speed. Song et al. \cite{song2016efficient} demonstrate the effectiveness of mmap with fast storage devices in Linux, while Malliotakis et al. \cite{malliotakis2021hugemap} introduce HugeMap, a custom mmap path using huge pages for improved throughput and reduced system time.

Papagiannis et al. \cite{papagiannis2020optimizing} propose FastMap, an mmap alternative designed for multi-core servers, reducing synchronization overhead and increasing device queue depth. Our experiments show FastMap's scalability to 80 cores, providing significant IOPS and throughput improvements compared to traditional mmap, especially with Optane SSDs.

Essen et al. \cite{van2015di} present DI-MMAP, a high-performance runtime that outperforms Linux mmap in memory-mapping large external datasets. Papagiannis et al. \cite{papagiannis2021memory} contribute Aquila, a library OS offering mmio compatibility, DRAM I/O cache customization, and reduced I/O overhead. Aquila demonstrates substantial benefits in key-value stores and graph processing applications, outperforming Linux mmap in both cache management and execution time.

Alverti et al. \cite{alverti2022daxvm} propose DaxVM, extending OS virtual memory and file system layers for a fast and scalable DAX-mmap interface. DaxVM achieves higher throughput than default mmap for the Apache webserver and PMem-optimized key-value stores, showcasing its relevance beyond byte-addressable storage.

Song et al. \cite{song2012low} investigate the Linux virtual memory subsystem and mmap() I/O path, proposing optimizations to reduce mmap() I/O overhead. Their solution ensures faster mmio compared to read-write I/O under high cache-hit ratios.
Imamura and Yoshida \cite{imamura2019poster} present AR-MMAP, an asynchronous read method to reduce write latency in memory-mapped files. Li et al. \cite{li2019userland} propose CO-PAGER, a userspace memory service, bypassing kernel I/O stacks for efficient memory paging. Feng et al. \cite{feng2023tricache} introduce TriCache, a cache mechanism for in-memory programs to process out-of-core datasets without code modification.

Leis et al. \cite{leis2023virtual} propose vmcache, a buffer manager using hardware-supported virtual memory for page translation, ensuring flexible, efficient, and scalable buffer management on multi-core CPUs and fast storage devices. Their second contribution, exmap, addresses performance bottlenecks in page table manipulation on Linux, collectively providing a comprehensive solution.
%%




%% MMAP
Song et al. \cite{song2012low} examine linux virtual memory subsystem and mmap() I/O path to figure out the influence of low-latency storage devices on the existing virtual memory subsystem. Also, we suggest some optimization policies to reduce the overheads of mmap() I/O and implement the prototype in a recent Linux kernel. Our solution guarantees that 1) memory-mapped I/O will be several times faster than read-write I/O when cache-hit ratio becomes high, and 2) the former will show at least the performance of the latter even when cache-miss frequently occurs and the overhead of mapping/unmapping pages becomes significant, which are not achievable by the existing virtual memory subsystem.

%% MMAP
Essen et al. \cite{van2015di} present DI-MMAP, a high-performance runtime that memory-maps large external data sets into an application’s address space and shows better performance than the Linux mmap system call.

%% MMAP
Song et al. \cite{song2016efficient} optimize Linux's memory mapped IO (mmio) for fast storage devices. Their results indicate that mmap can be used effectively with fast storage.

%% MMAP
Malliotakis et al. \cite{malliotakis2021hugemap} present HugeMap, a custom mmio path in the Linux kernel that uses huge pages for file-backed mappings to accelerate applications with sequential I/O access patterns or large I/O operations. Their experiments show up to higher throughput and lower system time, compared to regular page configurations.

%% MMAP
Imamura and Yoshida \cite{imamura2019poster} propose AR-MMAP that is an asynchronous read method to reduce the write latency of applications applying memory-mapped files. It hides the read latency of block devices by reading the corresponding pages from block devices in background. As AR-MMAP requires the assistance of applications to guarantee data consistency, we modify an in-memory key-value store as a use case. We implement AR-MMAP in a Linux kernel and evaluate its performance on a server containing two types of SSDs. The evaluation results demonstrate that it reduces the execution time compared to a default kernel.

%% MMAP
Li et al. \cite{li2019userland} propose CO-PAGER, which is a lightweight userspace memory service. CO-PAGER consists of a minimal kernel module and a userspace component. The userspace component handles (redirected) page faults, performs memory management and I/O operations and accesses NVM storage directly. The kernel module is used to update memory mapping between user and kernel space. In this way CO-PAGER can bypass the deep kernel I/O stacks and provide a flexible/customizable and efficient memory paging service in userspace. We provide a general programming interface to use the CO-PAGER service. In our experiments, we also demonstrate how the CO-PAGER approach can be applied to a MapReduce framework and improves performance for data-intensive applications.

%% MMAP
Papagiannis et al. \cite{papagiannis2020optimizing} propose FastMap, an alternative design for the memory-mapped I/O path in Linux that provides scalable access to fast storage devices in multi-core servers, by reducing synchronization overhead in the common path. FastMap also increases device queue depth, an important factor to achieve peak device throughput. Our experimental analysis shows that FastMap scales up to 80 cores and provides up to 11.8× more IOPS compared to mmap using null\_blk. Additionally, it provides up to 5.27x higher throughput using an Optane SSD. We also show that FastMap is able to saturate state-of-the-art fast storage devices when used by a large number of cores, where Linux mmap fails to scale.

%% MMAP
Papagiannis et al. \cite{papagiannis2021memory} present Aquila, a library OS that allows applications to reduce I/O overhead by customizing the memory-mapped I/O (mmio) path for files or storage devices. Compared to Linux mmap, Aquila (a) offers full mmio compatibility and protection to minimize application modifications, (b) allows applications to customize the DRAM I/O cache, its policies, and access to storage devices, and (c) significantly reduces I/O overhead. Aquila achieves its mmio compatibility, flexibility, and performance by placing the application in a privileged domain, non-root ring 0. We show the benefits of Aquila in two cases: (a) Using mmio in key-value stores to reduce I/O overhead and (b) utilizing mmio in graph processing applications to extend the memory heap over fast storage devices. Aquila requires 2.58× fewer CPU cycles for cache management in RocksDB, compared to user-space caching and read/write system calls and results in 40\% improvement in request throughput. Finally, we use Ligra, a graph processing framework, to show the efficiency of Aquila in extending the memory heap over fast storage devices. In this case, Aquila results in up to 4.14× lower execution time compared to Linux mmap.

%% MMAP
Alverti et al. \cite{alverti2022daxvm} propose DaxVM, a design that extends the OS virtual memory and file system layers leveraging persistent memory attributes to provide a fast and scalable DAX-mmap interface. DaxVM eliminates paging costs through pre-populated file page tables, supports faster and scalable virtual address space management for ephemeral mappings, performs unmappings asynchronously, bypasses kernel-space dirty-page tracking support, and adopts asynchronous block pre-zeroing. We implement DaxVM in Linux and the ext4 file system targeting xS6-64 architecture. DaxVM mmap achieves 4.9x higher throughput than default mmap for the Apache webserver and up to 1.5x better performance than read system calls. It provides similar benefits for text search. It also provides fast boot times and up to 2.95x better throughput than default mmap for PMem-optimized key-value stores running on a fragmented ext4 image. Despite designed for direct access to byte-addressable storage, various aspects of DaxVM are relevant for efficient access to other high performant storage mediums.

%% MMAP
Feng et al. \cite{feng2023tricache} propose TriCache, a cache mechanism that enables in-memory programs to efficiently process out-of-core datasets without requiring any code rewrite. It provides a virtual memory interface on top of the conventional block interface to simultaneously achieve user transparency and sufficient out-of-core performance. A multi-level block cache design is proposed to address the challenge of per-access address translations required by a memory interface. It can exploit spatial and temporal localities in memory or storage accesses to render storage-to-memory address translation and page-level concurrency control adequately efficient for the virtual memory interface.

%% MMAP
Leis et al. \cite{leis2023virtual} propose vmcache, a buffer manager design that instead uses hardware-supported virtual memory to translate page identifiers to virtual memory addresses. In contrast to existing mmap-based approaches, the DBMS retains control over page faulting and eviction. Our design is portable across modern operating systems, supports arbitrary graph data, enables variable-sized pages, and is easy to implement. One downside of relying on virtual memory is that with fast storage devices the existing operating system primitives for manipulating the page table can become a performance bottleneck. As a second contribution, we therefore propose exmap, which implements scalable page table manipulation on Linux. Together, vmcache and exmap provide flexible, efficient, and scalable buffer management on multi-core CPUs and fast storage devices.




%% MMAP LOAD
Graph and sparse matrix systems are highly tuned, able to run complex graph analytics in fractions of seconds on billion-edge graphs. For both developers and researchers, the focus has been on computational kernels and not end-to-end runtime. Despite the significant improvements that modern hardware and operating systems have made towards input and output, these can still become application bottlenecks. Unfortunately, on high-performance shared-memory graph systems running billion-scale graphs, reading the graph from file systems easily takes over 2000x longer than running the computational kernel. This slowdown causes both a disconnect for end users and a loss of productivity for researchers and developers. Gabert and Çatalyürek \cite{gabert2021pigo} close the gap by providing a simple to use, small, header-only, and dependency-free C++11 library that brings I/O improvements to graph and matrix systems. Using our library, we improve the end-to-end performance for state-of-the-art systems significantly-in many cases by over 40x.

%% MMAP GRAPH
Lin et al. \cite{lin2014mmap} use the fundamental memory mapping capability of all modern hardware to create fast and scalable graph algorithms. They are able to to process 6.6 billion edge Yahoo web graph faster than other graph processing frameworks, such as TurboGraph and GraphChi, with reduced complexity - both in terms of simpler data structures, and fewer lines of code. They benefit from existing page replacement policies, such as LRU.

%% MMAP GRAPH
Wang et al. \cite{wang2019lgraph} present LGraph, a unified data model and API for productive open-source hardware design. Key features of LGraph include a
unified data model and API, a fast memory mapped library design, integration with third-party tools and hierarchical design traversal for third-party tools.

%% MMAP GRAPH
Kim and Swanson \cite{kim2022blaze} introduce Blaze, a new out-of-core graph processing system optimized for ultra-low-latency SSDs. Blaze offers high-performance out-of-core graph analytics by constantly saturating these fast SSDs with a new scatter-gather technique called online binning that allows value propagation among graph vertices without atomic synchronization. Blaze offers succinct APIs to allow programmers to write efficient out-of-core graph algorithms without the burden to manage complex IO executions. Our evaluation shows that Blaze outperforms current out-of-core systems by a wide margin on seven datasets and a set of representative graph queries on Intel Optane SSD.

%% MMAP GRAPH
Han et al. \cite{han2013turbograph} propose a general, disk-based graph engine called TurboGraph to process billion-scale graphs very efficiently by using modern hardware on a single PC. TurboGraph is the first truly parallel graph engine that exploits 1) full parallelism including multi-core parallelism and FlashSSD IO parallelism and 2) full overlap of CPU processing and I/O processing as much as possible. Specifically, we propose a novel parallel execution model, called pin-and-slide. TurboGraph also provides engine-level operators such as BFS which are implemented under the pin-and-slide model. Extensive experimental results with large real datasets show that TurboGraph consistently and significantly outperforms Graph-Chi by up to four orders of magnitude! Our implementation of TurboGraph is available at ``http://wshan.net/turbograph" as executable files.

%% MMAP GRAPH
Zhou and Hoffmann \cite{zhou2018graphz} present two innovations that improve the performance of software frameworks for out-of-core graph analytics. The first is degree-ordered storage, a new storage format that dramatically lowers book-keeping overhead when graphs are larger than memory. The second innovation replaces existing static messages with novel ordered dynamic messages which update their destination immediately, reducing both the memory required for intermediate storage and IO pressure. We implement these innovations in a framework called GraphZ-which we release as open source-and we compare its performance to two state-of-the-art out-of-core graph frameworks. For graphs that exceed memory size, GraphZ's harmonic mean performance improvements are 1.8-8.3× over existing solutions.

%% MMAP GRAPH
Liu and Huang \cite{liu2017graphene} strive to achieve an ambitious goal of achieving ease of programming and high IO performance (as in-memory processing) while maintaining graph data on disks (as external memory processing). To this end, we have designed and developed Graphene that consists of four new techniques: an IO request centric programming model, bitmap based asynchronous IO, direct hugepage support, and data and workload balancing. The evaluation shows that Graphene can not only run several times faster than several external-memory processing systems, but also performs comparably with in-memory processing on large graphs.
