Graph processing frameworks are software systems designed to efficiently analyze and manipulate graph-structured data, facilitating tasks such as traversal, analytics, and algorithm implementation on various computational architectures. Ligra \cite{shun2013ligra} is a lightweight framework designed for shared-memory parallel/multicore machines, simplifying the development of graph traversal algorithms with two simple routines for edge and vertex mapping. The Galois system \cite{nguyen2013lightweight} advocates for a general-purpose infrastructure supporting fine-grain tasks, speculative execution, and application-specific task scheduling control. Gunrock \cite{wang2016gunrock} tackles irregular data access and control flow issues on GPUs by introducing a data-centric abstraction focused on vertex or edge frontier operations, and enables rapid development of new graph primitives with minimal GPU programming knowledge. Hornet \cite{busato2018hornet} provides a scalable GPU implementation without requiring data reallocation during evolution, targeting dynamic data problems. While frameworks such as these have demonstrated efficient computation on billion-scale graphs, many of them persist with sequential I/O.

Hornet \cite{busato2018hornet} reads a graph in the Matrix Market format using the stream operator on \texttt{ifstream}, and then converts it to CSR using \texttt{COOtoCSR()} (in the \texttt{graph::GraphBase::read()} function). The \texttt{COOtoCSR()} function then calculates the out-degrees of each vertex and computes the prefix sum (using \texttt{std::partial\_sum()}) to determine the offsets of outgoing edges in the CSR representation, fills in the CSR arrays ($\_out\_edges$ and $\_out\_offsets$) based on the sorted or randomized COO edges, and if the graph is directed, it also calculates the in-degrees and fills in the corresponding arrays ($\_in\_edges$ and $\_in\_offsets$). In Gunrock \cite{wang2016gunrock}, the \texttt{io::matrix\_market\_t::lo} \texttt{ad()} function handles reading of graphs in the Matrix Market format. It uses \texttt{fscanf()} to read the entries from the input file into three separate \texttt{vector}s, which are then passed to the \texttt{format::csr} \texttt{\_t::from\_coo()} function to generate a CSR. In GraphBLAST \cite{yang2022graphblast}, the \texttt{read} \texttt{Mtx()} function is used to read graphs in the Matrix Market format. It uses \texttt{fscanf()} to read tuples/entries, and pushes then to a \texttt{vector}. It then removes self-loops, and does a custom sort of the entries. The GAP Benchmark Suite \cite{beamer2015gap} uses \texttt{ifstream} with \texttt{getline()} and stream operator to read into COO. In each of these frameworks, all of the operations are performed sequentially.

Incorporation of Memory-Mapped I/O holds promise for improving graph loading efficiency of frameworks. A number of research studies have worked on optimizing the performance of memory mapped IO. The approaches include using a lightweight userspace memory service \cite{li2019userland}, huge pages \cite{malliotakis2021hugemap}, and asynchronous techniques \cite{imamura2019poster}. A large number of works have optimized memory mapped IO for fast low-latency storage devices \cite{song2012low, song2016efficient, papagiannis2020optimizing, papagiannis2021memory, alverti2022daxvm, leis2023virtual}. Essen et al. \cite{van2015di} and Feng et al. \cite{feng2023tricache} focus on enabling programs to efficiently process out-of-core datasets through memory mapping.

In NetworKit \cite{staudt2016networkit}, the \texttt{MatrixMarketReader::read()} function handles reading of graphs in the Matrix Market format. The function uses \texttt{istream} to read each entry using the stream operator, which is then pushed to a \texttt{vector}. This vector is then passed to \texttt{CSRMatrix}, which sequentially adds the entries in the vector to a CSR. Since the offsets array of the CSR must start with a zero, it puts a zero at the end and rotates the offsets array. We now discuss the \texttt{EdgeListReader::read()} function, which handles reading of graphs in COO/Edgelist format, and returns a \texttt{Graph}. It maps the file to memory with the \texttt{MemoryMappedFile} class, reads source/target vertex IDs with \texttt{strtol()}, edge weights with \texttt{strtod()}, conditionally adjusts the number of nodes at each step with \texttt{graph.add} \texttt{Nodes()}, and conditionally adds an edge to the graph with \texttt{graph.} \texttt{addEdge()}. However, the function sequential, and adding each edge to the graph is expensive, i.e., $O(D)$ (where $D$ is the average degree). Further, the \texttt{MemoryMappedFile} class does not use \texttt{madvice()} to recommend a memory access pattern to the OS.\ignore{The call to \texttt{mmap()} uses \texttt{PROT\_READ} and \texttt{MAP\_PRIVATE}.}

Gabert and Çatalyürek \cite{gabert2021pigo} observe that even on high-performance shared-memory graph systems running billion-scale graphs, reading the graph from file systems takes multiple orders of magnitude longer than running the computational kernel. To address this issue, they propose PIGO, a high-performance parallel graph loader that brings I/O improvements to graph systems.

PIGO employs the \texttt{O\_DIRECT} flag in Linux to open files and maps the entire file to memory using \texttt{mmap()}, with \texttt{MAP\_SHARED} and \texttt{MAP\_NORESERVE} flags. It additionally utilizes \texttt{madvice()} with the \texttt{MADV\_WILLNEED} flag. In the \texttt{COO::read\_mm\_()} function, PIGO disregards MatrixMarket attributes. The authors of PIGO plan to fix this in the future. PIGO proceeds to read the first line of the file, containing information about rows, columns, and the number of edges. For integer reading, it navigates the file reader pointer to the next digit, reads an integer (using custom code), and then moves to the end-of-line. Using the \texttt{COO::read\_el\_()} function, PIGO employs a two-pass approach. In the first pass, it counts newlines to allocate storage and copies values accordingly, splitting the file into equal parts for each thread. Each thread adjusts its boundaries to eliminate overlap and performs a prefix sum to determine write addresses. In the second pass, PIGO iterates through the file again, parsing and populating the source/target vertex IDs into the Edgelist. Memory allocation for \texttt{src}, \texttt{dst}, and \texttt{weights} is performed separately using \texttt{std::vector::resize}.

\ignore{
PIGO opens a file with \texttt{O\_DIRECT} flag in Linux, and maps the entire file to memory with \texttt{mmap()} using \texttt{MAP\_SHARED} and \texttt{MAP\_NORESERVE} flags, and uses \texttt{madvice()} with \texttt{MADV\_WILLNEED} flag. In the \texttt{COO::read\_mm\_()} PIGO simply ignores MatrixMarket attributes. The authors of PIGO plan to fix this in the future. PIGO then reads the first line, which includes the number of rows, columns, and the number of edges\ignore{(nnz)}. For reading integers it first moves the file reader pointer to the next digit\ignore{(custom code)}, reads an integer (custom code), and finally moves to the end-of-line. PIGO then proceeds to read the Edgelist with the \texttt{COO::read\_el\_()} function. In \texttt{COO::read\_el\_()}, it takes two passes. In the first pass, it counts the number of newlines in the part of text assigned to each thread, to determine how to allocate storage, and to copy over the values appropriately. PIGO splits the file into equal-sizes parts, where each thread operates on its own part of the file. Each thread then adjusts the boundaries of its part, in order to eliminate overlapping entries. PIGO then use a loop, for each thread, to count the number of entries in the part assigned to each thread. This is then populated to an offsets array, upon which they a prefix sum performed to obtain the address each thread must write to (single threaded).\ignore{The \texttt{read\_coord\_entry\_()} function uses condition to check for checking for end of stream after scanning an integer, during counting coords, and uses some redundant checks with \texttt{move\_to\_next\_int()} and \texttt{read\_int()}, conditions for updating \texttt{max\_row} and \texttt{max\_col}, when reading each coord entry.} In the second pass, PIGO iterates through the input file again, but this time, parsing and populating the source/target vertex IDs into the Edgelist.\ignore{It then does a reduction on \texttt{max\_row} and \texttt{max\_col}.} PIGO allocates memory separately, for \texttt{src}, \texttt{dst}, and \texttt{weights} --- and the allocation is done using \texttt{std::vector::resize} (not \texttt{mmap}).}

After reading a COO, PIGO converts the COO to CSR format with the \texttt{convert\_coo\_()} function. Alternatively, its uses the \texttt{read\_graph\_()} function, if the file type is specified as \texttt{GRAPH}. In \texttt{convert\_coo\_()}, PIGO first allocates space for the CSR. It then uses a multi pass algorithm. First, it count each vertex's degree, and allocate space appropriately. Next, it goes through the degrees and change them to offsets. Finally, PIGO goes through the COO, and copies memory. PIGO uses plain OpenMP for, thus a \textit{static} schedule with a chunk size of $1$. It atomically increments degrees, on each thread, in a global shared vector. PIGO then does a sequential prefix sum to find the offset to write to in the offsets array (local degree count). It then does a sequential prefix sum (on the vertex range of each thread) to obtain the offsets. The prefix sum does not start with a zero, so PIGO adjusts for this, and patches the last offset. PIGO then treats the degrees computed earlier as the remaining vertices, showing the current copy position. It then copies over the COO, again with a plain OpenMP for with \textit{static} schedule of size $1$. Adding to the CSR is done with atomic\ignore{capture} instructions. Finally temporary arrays are freed.

\ignore{cuDF employs a GPU-accelerated parsing algorithm to efficiently read and interpret the CSV data within each chunk. This algorithm is optimized for GPU architecture and takes advantage of parallelism to process data quickly.}
