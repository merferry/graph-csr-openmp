Graph processing frameworks are software systems designed to efficiently analyze and manipulate graph-structured data, facilitating tasks such as traversal, analytics, and algorithm implementation on various computational architectures. Ligra \cite{shun2013ligra} is a lightweight framework designed for shared-memory parallel/multicore machines, simplifying the development of graph traversal algorithms with two simple routines for edge and vertex mapping. The Galois system \cite{nguyen2013lightweight} advocates for a general-purpose infrastructure supporting fine-grain tasks, speculative execution, and application-specific task scheduling control. Gunrock \cite{wang2016gunrock} tackles irregular data access and control flow issues on GPUs by introducing a data-centric abstraction focused on vertex or edge frontier operations, and enables rapid development of new graph primitives with minimal GPU programming knowledge. Hornet \cite{busato2018hornet} provides a scalable GPU implementation without requiring data reallocation during evolution, targeting dynamic data problems.

While these frameworks have demonstrated efficient computation on billion-scale graphs, they persist with sequential I/O and do not fully exploit the capabilities of modern Hard Disk Drives (HDDs), Redundant Array of Independent Disks (RAID) controllers, Non-Volatile Memory (NVM), and file system caches \cite{gabert2021pigo}. Incorporation of Memory-Mapped I/O holds promise for improving graph loading efficiency of frameworks. A number of research studies have worked on optimizing the performance of memory mapped IO. The approaches include using a lightweight userspace memory service \cite{li2019userland}, huge pages \cite{malliotakis2021hugemap}, and asynchronous techniques \cite{imamura2019poster}. A large number of works have optimized memory mapped IO for fast low-latency storage devices \cite{song2012low, song2016efficient, papagiannis2020optimizing, papagiannis2021memory, alverti2022daxvm, leis2023virtual}. Essen et al. \cite{van2015di} and Feng et al. \cite{feng2023tricache} focus on enabling programs to efficiently process out-of-core datasets through memory mapping.

Incorporation of Memory-Mapped I/O holds promise for improving graph loading efficiency of frameworks. Memory mapping is a mechanism that maps a file or part of a file into the virtual memory space, so that files on the disk can be accessed as if they were in memory \cite{lin2014mmap}. There are many advantages of using memory mapping, especially when processing large files. - Reading from and writing to a memory-mapped file do not require the data to be copied to and from a user-space buffer while standard read/write do. - Aside from any potential page faults, reading from and writing to a memory-mapped file do not incur any overhead due to context switching. - When multiple processes map the same data into memory, they can access that data simultaneously. Read-only and shared writable mappings are shared in their entirety; private writable mappings may have their not-yet-COW (copy-on-write) pages shared \cite{lin2014mmap}. A number of research studies have worked on optimizing the performance of memory mapped IO. The approaches include using a lightweight userspace memory service \cite{li2019userland}, huge pages \cite{malliotakis2021hugemap}, and asynchronous techniques \cite{imamura2019poster}. A large number of works have optimized memory mapped IO for fast low-latency storage devices \cite{song2012low, song2016efficient, papagiannis2020optimizing, papagiannis2021memory, alverti2022daxvm, leis2023virtual}. Essen et al. \cite{van2015di} and Feng et al. \cite{feng2023tricache} focus on enabling programs to efficiently process out-of-core datasets through memory mapping. Lin et al. \cite{lin2014mmap} use memory mapping capability of all modern hardware to create fast and scalable graph algorithms. They are able to to process $6.6$ billion edge Yahoo web graph faster than other graph processing frameworks, such as TurboGraph and GraphChi, with reduced complexity - both in terms of simpler data structures, and fewer lines of code. They benefit from existing page replacement policies, such as LRU.

Lin et al. \cite{lin2014mmap} use memory mapping capability of all modern hardware to create fast and scalable graph algorithms. They are able to to process 6.6 billion edge Yahoo web graph faster than other graph processing frameworks, such as TurboGraph and GraphChi, with reduced complexity - both in terms of simpler data structures, and fewer lines of code. They benefit from existing page replacement policies, such as LRU. Gabert and Çatalyürek \cite{gabert2021pigo} observe that even on high-performance shared-memory graph systems running billion-scale graphs, reading the graph from file systems takes multiple orders of magnitude longer than running the computational kernel. This slowdown causes both a disconnect for end users and a loss of productivity for researchers and developers. Gabert and Çatalyürek \cite{gabert2021pigo} close the gap by providing a simple to use, small, header-only, and dependency-free C++11 library that brings I/O improvements to graph and matrix systems.

External memory processing in graph systems involves efficiently managing and processing large-scale graphs by leveraging disk storage, enabling effective handling of datasets that exceed the available in-memory capacity. Han et al. \cite{han2013turbograph} propose the disk-based TurboGraph framework, which exploits multi-core and FlashSSD IO parallelism. They overlap CPU processing and I/O processing as much as possible, and propose a parallel execution model, called pin-and-slide. Liu and Huang \cite{liu2017graphene} develop the Graphene framework, which provides an IO request centric programming model, bitmap based asynchronous IO, direct hugepage support, and data and workload balancing. Zhou and Hoffmann \cite{zhou2018graphz} design the out-of-core GraphZ framework, which uses degree-ordered storage to lower book-keeping overhead for large graphs and ordered dynamic messages which update their destination immediately. Kim and Swanson \cite{kim2022blaze} introduce Blaze, an out-of-core graph processing system optimized for ultra-low-latency SSDs. Blaze saturates these SSDs with online binning, a scatter-gather technique that allows value propagation among graph vertices without atomic synchronization.




%% FLOW GO
Single-machine graph processing systems store and process a given graph in a single machine. Existing systems include in-memory systems like Ligra  \cite{shun2013ligra} and Galois \cite{kulkarni2007optimistic, nguyen2013lightweight} and out-of-core systems like GraphChi \cite{kyrola2012graphchi}, TurboGraph \cite{han2013turbograph}, X-stream \cite{roy2013x}, VENUS \cite{cheng2015venus} and so on \cite{chi2016nxgraph, vora2016load, wang2013asynchronous}. Single-machine graph processing systems have high efficiency because of communication cost saving and fast convergence. However, the disadvantage is weak scalability due to limited hardware resources.

Disk-based processing of large graphs enables processing to scale beyond the available main memory in both single machine \cite{ai2017squeezing, han2013turbograph, jun2018grafboost, kyrola2012graphchi, lin2014mmap, ma2017garaph, maass2017mosaic, roy2013x, zhang2018wonderland, zheng2015flashgraph, zhu2015gridgraph} and cluster based \cite{roy2015chaos} processing environments \cite{vora2019lumos}.

The major performance bottleneck of out-of-core systems is disk I/O. Therefore, improving the locality of disk I/O has been the main optimization goal \cite{ai2018clip}. It is necessary to use multiple disks in parallel efficiently in order to achieve high bandwidth \cite{sanders2003fast}. SSD file systems are designed for high IOPS \cite{zheng2015flashgraph}.

%% Note some are heterogeneous, and some are GPU-focused.
Distributed graph processing systems incur overheads of partitioning and distribution of the large graph over a cluster of nodes \cite{najeebullah2014bishard, wang2021scaleg}. Disk-based graph computation on just a single shared-memory machine/PC can be as competitive as cluster-based computing systems on large-scale problem \cite{roy2013x, cheng2015venus}. To overcome these problems, a number of disk-based out-of-memory graph processing systems/frameworks, namely, GraphChi \cite{kyrola2012graphchi}, TurboGraph \cite{han2013turbograph}, X-Stream \cite{roy2013x}, BiShard Parallel Processor (BSPP) \cite{najeebullah2014bishard}, MMap \cite{lin2014mmap}, VENUS \cite{cheng2015venus}, GridGraph \cite{zhu2015gridgraph}, FlashGraph \cite{zheng2015flashgraph}, NXgraph \cite{chi2016nxgraph}, Garaph (heterogeneous) \cite{ma2017garaph}, MOSAIC (heterogeneous) \cite{maass2017mosaic}, Redio \cite{wu2018redio}, Clip \cite{ai2018clip}, ScaleG \cite{wang2021scaleg}, and few heterogeneous ones, such as, X and Y, and a few solutions to specific graphs problems, such as OPT \cite{kim2014opt} for triangulation and DUALSIM \cite{kim2016dualsim} for subgraph enumeration, have been proposed, which focus on loading of large graphs stored in binary formats. However, there has been is little focus of fast loading on graphs in text-based formats, which are readily exchangeable. Loading graphs is text-based formats, such as the MatrixMarket format is challenging because ...

%% RANDOM WALKS
Li et al. \cite{li2022efficient} introduce an I/O-efficient disk-based graph system for the scalable second-order random walk of large graphs, called GraSorw. First, they develop a bi-block execution engine that converts random I/Os into sequential I/Os by applying a new triangular bi-block scheduling strategy, the bucket-based walk management, and the skewed walk storage. Second, to improve the I/O utilization, they design a learning-based block loading model to leverage the advantages of the full-load and on-demand load methods.

%% NEW RELATED WORKS
PIGO supports reading from file in parallel, with details of their secondary storage (NVMe HDD). PIGO has support for COO (coordinate-addressed matrices or graphs) and CSR (compressed sparse row matrices or graphs). COO is able to read a variety of input formats (e.g., matrix market) and exposes the read data as an edge list. The edge list is stored as multiple arrays, one for the x elements and one for the y elements (e.g., src and dst in graphs.). CSR is used to represent sparse matrices and graphs. In many cases, this is the desired format for a graph or matrix in memory. When sparse graphs or matrices are delivered in COO formats, such as matrix market or edge lists, they are frequently converted to CSR.

%% READING PIGO CODE (File)
PIGO opens a file with \texttt{O\_DIRECT} flag in Linux, and maps the entire file to memory with \texttt{mmap()} using \texttt{MAP\_SHARED} and \texttt{MAP\_NORESERVE} flags, and uses \texttt{madvice()} with \texttt{MADV\_WILLNEED} flag.

%% READING PIGO CODE (COO)
In the \texttt{COO::read\_mm\_()} PIGO simply ignores MatrixMarket attributes (note that this will cause them to report lower runtimes for symmetric graphs). They plan to them in the future. They then read the first file, which includes the number of rows, columns, and edges (nnz). For reading integers they first move their file reader pointer to the next it (custom code), read an int (custom code), and finally move to EOL. They then proceed to read the edgelist with \texttt{COO::read\_el\_()}. In \texttt{COO::read\_el\_()}, they take two passes, first to count the number of newlines to determine how to allocate storage, second, copy over the values appropriately. They do not split the file into chunks, but split the file into parts, where each file operates on its own part. Then, at each thread, they move off overlapping entries. They then use a loop to count all coord entries in the file. This is then populated to an offsets array, upon which they perform prefix sum to obtain the address each thread must write to (single threaded). The \texttt{read\_coord\_entry\_()} function uses condition to check for checking for end of stream after scanning an integer, during counting coords, and uses some redundant checks with \texttt{move\_to\_next\_int()} and \texttt{read\_int()}, conditions for updating \texttt{max\_row} and \texttt{max\_col}, when reading each coord entry. In pass 2, they iterate through again, but now copying out the integers. Then then do a reduction on \texttt{max\_row} and \texttt{max\_col}. They allocate memory separately, for \texttt{src}, \texttt{dst}, and \texttt{weights} --- and they allocation is done using \texttt{std::vector::resize} (not \texttt{mmap}).

%% READING PIGO CODE (CSR)
After reading a COO, they convert the COO to CSR format with \texttt{convert\_coo\_()}. Alternatively, they use \texttt{read\_graph\_()}, if the file type is specified as \texttt{GRAPH}. In \texttt{convert\_coo\_()} function, they first allocate space for the CSR. They then use a multi pass algorithm. First, they need to count each vertex's degree and allocate the space appropriately. Next, they go through the degrees and change them to offsets. Finally, they need to go through the COO and copy memory. They use plain OpenMP for, thus a static schedule with a chunk size of $1$. They atomically increment degrees, on each thread, in a global shared vector. They then do a sequential prefix sum to find the offset to write to in the offsets array (local degree count). They then do a sequential prefix sum (on the vertex range of each thread) to obtain the offsets. Their prefix sum does not start with a zero, so they adjust for this, and patch the last offset (not exactly clear to me). They then treat the degrees computed earlier as the remaining vertices, showing the current copy position. They then copy over the COO over, again an OpenMP for with static schedule of size $1$. Adding to the CSR is done with atomic capture instructions. Finally temporary arrays are freed.

%% ON HORNET
Hornet sequentially reads the MatrixMarket file using the stream operator on \texttt{ifstream}, and then converts it to CSR using \texttt{COOtoCSR()} (in \texttt{graph::GraphBase::read()}). \texttt{COOtoCSR()} calculates the out-degrees of each vertex and computes the prefix sum to determine the offsets of outgoing edges in the CSR representation, fills in the CSR arrays (\_out\_edges and \_out\_offsets) based on the sorted or randomized COO edges, and if the graph is directed and reverse, it also calculates the in-degrees and fills in the corresponding arrays (\_in\_edges and \_in\_offsets). It then deallocates temporary memory arrays and releases memory if the graph is no longer stored in COO format (using \texttt{std::partial\_sum(), the code is sequential}).
% If the graph is directed and reverse, it calculates both out-degrees and in-degrees.
% If the graph is directed, it calculates only out-degrees.
% If the graph is degree-directed, it adjusts the degrees and edges accordingly.
% If specified (_prop.is_sort()), it sorts the COO edges.
% If specified (_prop.is_rm_singleton()) and the graph is reverse, it removes singleton vertices and relabels vertices accordingly.
% Finally, it constructs the CSR arrays _out_offsets and _out_edges, and if the graph is directed and reverse, it also constructs _in_offsets and _in_edges.

% COO to CSR Conversion:
% The function then converts the COO representation to CSR. Depending on the graph's structure, it calculates either out-degrees or both out-degrees and in-degrees.
% If directed by degree is enabled, the function modifies the COO edges to consider only edges with smaller source vertex IDs in case of equal out-degrees.
% If sorting is enabled and the graph is not directed to undirected or randomization is enabled, the COO edges array is sorted.
% If singleton removal is enabled and the graph is directed and reversed, singleton vertices (vertices with zero out-degree or zero in-degree) are removed by relabeling vertices and updating edges accordingly.
% Finally, the function calculates the CSR arrays _out_offsets and _in_offsets and fills the _out_edges and _in_edges arrays accordingly.





%% NetworKit
In NetworKit \cite{staudt2016networkit}, the \texttt{MatrixMarketReader::read()} function handles reading of graphs in the Matrix Market format. The function uses \texttt{istream} to read each entry using the stream operator \texttt{>>}, which are then pushed to a \texttt{vector}. This vector is then passed to \texttt{CSRMatrix}, which sequentially adds the entries in the vector to a CSR. Since the offsets array of the CSR must start with a zero, it puts a zero at the end and rotates the offsets array.

We now discuss the \texttt{EdgeListReader::read()} function, which handles reading of graphs in Edgelist format, and returns a \texttt{Graph}. It maps the file to memory with the \texttt{MemoryMappedFile} class, reads source/target vertex IDs with \texttt{strtol()}, edge weights with \texttt{strtod()}, conditionally adjusts the number of nodes at each step with \texttt{graph.addNodes()}, and conditionally adds an edge to the graph with \texttt{graph.addEdge()}. However, the function sequential, and adding each edge to the graph is expensive, with a time complexity of $O(D)$ (where $D$ is the average degree to the graph). Further, the \texttt{MemoryMappedFile} class does not use \texttt{madvice()} to recommend memory access pattern to the OS.\ignore{The call to \texttt{mmap()} uses \texttt{PROT\_READ} and \texttt{MAP\_PRIVATE}.}

%% Gunrock
In Gunrock, the \texttt{io::matrix\_market\_t::load()} function handles reading of graphs in the Matrix Market format. It uses \texttt{fscanf()} to read the entries from the input file into three separate \texttt{vector}s, which are then passed to the \texttt{format::csr\_t::from\_coo()} function to generate a CSR. All operations are sequential.

%% GraphBlast
In GraphBLAST \cite{yang2022graphblast}, the \texttt{readMtx()} function is used to read graphs in the Matrix Market format. It uses \texttt{fscanf()} to read tuples/entries, and pushes then to a \texttt{vector}. It then removes self-loops, and does a slow custom sort of the entries. All operations are sequential.

%% GAP BS
GAP Benchmark Suite \cite{beamer2015gap} uses \texttt{ifstream} with \texttt{getline()} and stream operator to read into COO format sequentially.

\ignore{cuDF employs a GPU-accelerated parsing algorithm to efficiently read and interpret the CSV data within each chunk. This algorithm is optimized for GPU architecture and takes advantage of parallelism to process data quickly.}
