Lin et al. \cite{lin2014mmap} use the fundamental memory mapping capability of all modern hardware to create fast and scalable graph algorithms. They are able to to process 6.6 billion edge Yahoo web graph faster than other graph processing frameworks, such as TorboGraph and GraphChi, with reduced complexity - both in terms of simpler data structures, and fewer lines of code. They benefit from existing page replacement policies, such as LRU.

Song et al. \cite{song2016efficient} optimize Linux's memory mapped IO (mmio) for fast storage devices. Their results indicate that mmap can be used effectively with fast storage.

Malliotakis et al. \cite{malliotakis2021hugemap} present HugeMap, a custom mmio path in the Linux kernel that uses huge pages for file-backed mappings to accelerate applications with sequential I/O access patterns or large I/O operations. Their experiments show up to higher throughput and lower system time, compared to regular page configurations.

Papagiannis et al. \cite{papagiannis2020optimizing} propose FastMap, an alternative design for the memory-mapped I/O path in Linux that provides scalable access to fast storage devices in multi-core servers, by reducing synchronization overhead in the common path. FastMap also increases device queue depth, an important factor to achieve peak device throughput. Our experimental analysis shows that FastMap scales up to 80 cores and provides up to 11.8× more IOPS compared to mmap using null\_blk. Additionally, it provides up to 5.27x higher throughput using an Optane SSD. We also show that FastMap is able to saturate state-of-the-art fast storage devices when used by a large number of cores, where Linux mmap fails to scale.

Essen et al. \cite{van2015di} present DI-MMAP, a high-performance runtime that memory-maps large external data sets into an application’s address space and shows better performance than the Linux mmap system call.

Wang et al. \cite{wang2019lgraph} present LGraph, a unified data model and API for productive open-source hardware design. Key features of LGraph include a
unified data model and API, a fast memory mapped library design, integration with third-party tools and hierarchical design traversal for third-party tools.

Papagiannis et al. \cite{papagiannis2021memory} present Aquila, a library OS that allows applications to reduce I/O overhead by customizing the memory-mapped I/O (mmio) path for files or storage devices. Compared to Linux mmap, Aquila (a) offers full mmio compatibility and protection to minimize application modifications, (b) allows applications to customize the DRAM I/O cache, its policies, and access to storage devices, and (c) significantly reduces I/O overhead. Aquila achieves its mmio compatibility, flexibility, and performance by placing the application in a privileged domain, non-root ring 0. We show the benefits of Aquila in two cases: (a) Using mmio in key-value stores to reduce I/O overhead and (b) utilizing mmio in graph processing applications to extend the memory heap over fast storage devices. Aquila requires 2.58× fewer CPU cycles for cache management in RocksDB, compared to user-space caching and read/write system calls and results in 40\% improvement in request throughput. Finally, we use Ligra, a graph processing framework, to show the efficiency of Aquila in extending the memory heap over fast storage devices. In this case, Aquila results in up to 4.14× lower execution time compared to Linux mmap.

Alverti et al. \cite{alverti2022daxvm} propose DaxVM, a design that extends the OS virtual memory and file system layers leveraging persistent memory attributes to provide a fast and scalable DAX-mmap interface. DaxVM eliminates paging costs through pre-populated file page tables, supports faster and scalable virtual address space management for ephemeral mappings, performs unmappings asynchronously, bypasses kernel-space dirty-page tracking support, and adopts asynchronous block pre-zeroing. We implement DaxVM in Linux and the ext4 file system targeting xS6-64 architecture. DaxVM mmap achieves 4.9x higher throughput than default mmap for the Apache webserver and up to 1.5x better performance than read system calls. It provides similar benefits for text search. It also provides fast boot times and up to 2.95x better throughput than default mmap for PMem-optimized key-value stores running on a fragmented ext4 image. Despite designed for direct access to byte-addressable storage, various aspects of DaxVM are relevant for efficient access to other high performant storage mediums.

Song et al. \cite{song2012low} examine linux virtual memory subsystem and mmap() I/O path to figure out the influence of low-latency storage devices on the existing virtual memory subsystem. Also, we suggest some optimization policies to reduce the overheads of mmap() I/O and implement the prototype in a recent Linux kernel. Our solution guarantees that 1) memory-mapped I/O will be several times faster than read-write I/O when cache-hit ratio becomes high, and 2) the former will show at least the performance of the latter even when cache-miss frequently occurs and the overhead of mapping/unmapping pages becomes significant, which are not achievable by the existing virtual memory subsystem.

Imamura and Yoshida \cite{imamura2019poster} propose AR-MMAP that is an asynchronous read method to reduce the write latency of applications applying memory-mapped files. It hides the read latency of block devices by reading the corresponding pages from block devices in background. As AR-MMAP requires the assistance of applications to guarantee data consistency, we modify an in-memory key-value store as a use case. We implement AR-MMAP in a Linux kernel and evaluate its performance on a server containing two types of SSDs. The evaluation results demonstrate that it reduces the execution time compared to a default kernel.

Li et al. \cite{li2019userland} propose CO-PAGER, which is a lightweight userspace memory service. CO-PAGER consists of a minimal kernel module and a userspace component. The userspace component handles (redirected) page faults, performs memory management and I/O operations and accesses NVM storage directly. The kernel module is used to update memory mapping between user and kernel space. In this way CO-PAGER can bypass the deep kernel I/O stacks and provide a flexible/customizable and efficient memory paging service in userspace. We provide a general programming interface to use the CO-PAGER service. In our experiments, we also demonstrate how the CO-PAGER approach can be applied to a MapReduce framework and improves performance for data-intensive applications.

Feng et al. \cite{feng2023tricache} propose TriCache, a cache mechanism that enables in-memory programs to efficiently process out-of-core datasets without requiring any code rewrite. It provides a virtual memory interface on top of the conventional block interface to simultaneously achieve user transparency and sufficient out-of-core performance. A multi-level block cache design is proposed to address the challenge of per-access address translations required by a memory interface. It can exploit spatial and temporal localities in memory or storage accesses to render storage-to-memory address translation and page-level concurrency control adequately efficient for the virtual memory interface.

Leis et al. \cite{leis2023virtual} propose vmcache, a buffer manager design that instead uses hardware-supported virtual memory to translate page identifiers to virtual memory addresses. In contrast to existing mmap-based approaches, the DBMS retains control over page faulting and eviction. Our design is portable across modern operating systems, supports arbitrary graph data, enables variable-sized pages, and is easy to implement. One downside of relying on virtual memory is that with fast storage devices the existing operating system primitives for manipulating the page table can become a performance bottleneck. As a second contribution, we therefore propose exmap, which implements scalable page table manipulation on Linux. Together, vmcache and exmap provide flexible, efficient, and scalable buffer management on multi-core CPUs and fast storage devices.






Graph processing frameworks
PIGO
