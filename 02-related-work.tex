Graph processing frameworks are software systems designed to efficiently analyze and manipulate graph-structured data, facilitating tasks such as traversal, analytics, and algorithm implementation on various computational architectures. Ligra \cite{shun2013ligra} is a lightweight framework designed for shared-memory parallel/multicore machines, simplifying the development of graph traversal algorithms with two simple routines for edge and vertex mapping. The Galois system \cite{nguyen2013lightweight} advocates for a general-purpose infrastructure supporting fine-grain tasks, speculative execution, and application-specific task scheduling control. Gunrock \cite{wang2016gunrock} tackles irregular data access and control flow issues on GPUs by introducing a data-centric abstraction focused on vertex or edge frontier operations, and enables rapid development of new graph primitives with minimal GPU programming knowledge. Hornet \cite{busato2018hornet} provides a scalable GPU implementation without requiring data reallocation during evolution, targeting dynamic data problems.

While these frameworks have demonstrated efficient computation on billion-scale graphs, they persist with sequential I/O and do not fully exploit the capabilities of modern Hard Disk Drives (HDDs), Redundant Array of Independent Disks (RAID) controllers, Non-Volatile Memory (NVM), and file system caches \cite{gabert2021pigo}. Incorporation of Memory-Mapped I/O holds promise for improving graph loading efficiency of frameworks. A number of research studies have worked on optimizing the performance of memory mapped IO. The approaches include using a lightweight userspace memory service \cite{li2019userland}, huge pages \cite{malliotakis2021hugemap}, and asynchronous techniques \cite{imamura2019poster}. A large number of works have optimized memory mapped IO for fast low-latency storage devices \cite{song2012low, song2016efficient, papagiannis2020optimizing, papagiannis2021memory, alverti2022daxvm, leis2023virtual}. Essen et al. \cite{van2015di} and Feng et al. \cite{feng2023tricache} focus on enabling programs to efficiently process out-of-core datasets through memory mapping.

Lin et al. \cite{lin2014mmap} use memory mapping capability of all modern hardware to create fast and scalable graph algorithms. They are able to to process 6.6 billion edge Yahoo web graph faster than other graph processing frameworks, such as TurboGraph and GraphChi, with reduced complexity - both in terms of simpler data structures, and fewer lines of code. They benefit from existing page replacement policies, such as LRU. Gabert and Çatalyürek \cite{gabert2021pigo} observe that even on high-performance shared-memory graph systems running billion-scale graphs, reading the graph from file systems takes multiple orders of magnitude longer than running the computational kernel. This slowdown causes both a disconnect for end users and a loss of productivity for researchers and developers. Gabert and Çatalyürek \cite{gabert2021pigo} close the gap by providing a simple to use, small, header-only, and dependency-free C++11 library that brings I/O improvements to graph and matrix systems.

\ignore{External memory processing in graph systems involves efficiently managing and processing large-scale graphs by leveraging disk storage, enabling effective handling of datasets that exceed the available in-memory capacity. Han et al. \cite{han2013turbograph} propose the disk-based TurboGraph framework, which exploits multi-core and FlashSSD IO parallelism. They overlap CPU processing and I/O processing as much as possible, and propose a parallel execution model, called pin-and-slide. Liu and Huang \cite{liu2017graphene} develop the Graphene framework, which provides an IO request centric programming model, bitmap based asynchronous IO, direct hugepage support, and data and workload balancing. Zhou and Hoffmann \cite{zhou2018graphz} design the out-of-core GraphZ framework, which uses degree-ordered storage to lower book-keeping overhead for large graphs and ordered dynamic messages which update their destination immediately. Kim and Swanson \cite{kim2022blaze} introduce Blaze, an out-of-core graph processing system optimized for ultra-low-latency SSDs. Blaze saturates these SSDs with online binning, a scatter-gather technique that allows value propagation among graph vertices without atomic synchronization.}
